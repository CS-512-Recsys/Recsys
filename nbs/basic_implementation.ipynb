{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc1d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from joblib import dump, load\n",
    "base_dir = Path('/Users/vinay/Projects/Recsys')\n",
    "if not base_dir:\n",
    "    base_dir = Path(os.getcwd())\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = base_dir/'data'/'archive'\n",
    "store_dir = base_dir/'artifacts'\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader as dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32abe8",
   "metadata": {},
   "source": [
    "- We have 20M `user-rating` data from 1995 to 2015.\n",
    "- To avoid `cold-start` problem I choose to split each user data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: I have taken a smaller size versions of the whole data  `partial`(with 1 lakh rows) and `sample`(few thousand rows). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522a8771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:53:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:31:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:33:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:32:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating            timestamp\n",
       "0       1        2     3.5  2005-04-02 23:53:47\n",
       "1       1       29     3.5  2005-04-02 23:31:16\n",
       "2       1       32     3.5  2005-04-02 23:33:39\n",
       "3       1       47     3.5  2005-04-02 23:32:07"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how a full data-set looks\n",
    "df = pd.read_csv(data_dir/'full_rating.csv')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6973d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       2        0     1.5\n",
       "1       3        1     5.0\n",
       "2       1        2     3.4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data = {'userId':[2,3,1],'movieId':[0,1,2],'rating':[1.5,5.0,3.4]}\n",
    "pd.DataFrame(dummy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdbc1a",
   "metadata": {},
   "source": [
    "## Code to split the data-sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106985c",
   "metadata": {},
   "source": [
    "Need not run as most likely you already have these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a992ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_sz(df,sz=10000):\n",
    "    all_users = sorted(list(set(df['userId'])))\n",
    "    data = []\n",
    "    start = 0\n",
    "    for user in tqdm(all_users):\n",
    "        temp = df[df['userId'] == user]\n",
    "        end = start+len(temp)\n",
    "        temp = copy.deepcopy(temp.reindex(range(start,end)))\n",
    "        start = end\n",
    "        data.append(temp)\n",
    "        if end > sz:\n",
    "            break\n",
    "    return pd.concat(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4f707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9602a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 90/138493 [00:01<31:08, 74.07it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_df = reduce_sz(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "45d6f8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▊                                   | 6742/138493 [01:28<28:56, 75.85it/s]\n"
     ]
    }
   ],
   "source": [
    "part_df = reduce_sz(df,sz=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d050b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv(data_dir/'sample_rating.csv',index=False)\n",
    "part_df.to_csv(data_dir/'partial_rating.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3fd0f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4685bb1e",
   "metadata": {},
   "source": [
    "# Splitting data into - train,valid,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909c9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "global seed \n",
    "import random\n",
    "seed = 0\n",
    "def set_seeds(seed=0):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # multi-GPU\n",
    "set_seeds(seed)\n",
    "    \n",
    "def get_data(data_dir,split:list,mode,data_type,save_to_disk=False):\n",
    "    #split = [0.6,0.5]\n",
    "    #mode = 'random' or 'seq_aware'\n",
    "    #save_to_disk = flag to specify if we also want to save the splitted data to disk to avoid future computation.\n",
    "    # We are currently using 'random' split-> for each user, we take all the rows(movies he rated ) and randomly split\n",
    "    #them between trn,vld,tst\n",
    "    trn,vld,tst = defaultdict(list),defaultdict(list),defaultdict(list)\n",
    "    df = pd.read_csv(data_dir/(data_type+'_'+'rating.csv'))\n",
    "    #Reorder rows\n",
    "    perm = random.sample(range(len(df)),len(df))\n",
    "    df = df.iloc[perm].reset_index()\n",
    "    df['date'] = pd.to_datetime(df['timestamp'])\n",
    "    user_ids = set(df['userId'])\n",
    "    for user in tqdm(user_ids):\n",
    "        tmp = df[df['userId'] == user]\n",
    "        if split == 'seq_aware':\n",
    "            tmp = tmp.sort_values(by='date')\n",
    "        sz = len(tmp)\n",
    "        #splitting-ids\n",
    "        t,v = int(split[0]*sz),int(split[0]*sz)+int((sz-int(split[0]*sz))*split[-1])\n",
    "        trn_ids,vld_ids,tst_ids = slice(0,t),slice(t,v),slice(v,sz)\n",
    "        \n",
    "        trn['user_id'].extend(tmp[trn_ids]['userId'].tolist())\n",
    "        trn['rating'].extend(tmp[trn_ids]['rating'].tolist())\n",
    "        trn['movie_id'].extend(tmp[trn_ids]['movieId'].tolist())\n",
    "        \n",
    "        \n",
    "        vld['user_id'].extend(tmp[vld_ids]['userId'].tolist())\n",
    "        vld['rating'].extend(tmp[vld_ids]['rating'].tolist())\n",
    "        vld['movie_id'].extend(tmp[vld_ids]['movieId'].tolist())\n",
    "        \n",
    "        tst['user_id'].extend(tmp[tst_ids]['userId'].tolist())\n",
    "        tst['rating'].extend(tmp[tst_ids]['rating'].tolist())\n",
    "        tst['movie_id'].extend(tmp[tst_ids]['movieId'].tolist())\n",
    "    trn,vld,tst = pd.DataFrame(trn),pd.DataFrame(vld),pd.DataFrame(tst)\n",
    "    if save_to_disk:\n",
    "        trn.to_csv(data_dir/(data_type+'_'+mode+'_trn.csv'))\n",
    "        vld.to_csv(data_dir/(data_type+'_'+mode+'_vld.csv'))\n",
    "        tst.to_csv(data_dir/(data_type+'_'+mode+'_tst.csv'))\n",
    "    return trn,vld,tst\n",
    "        \n",
    "    \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03118c54",
   "metadata": {},
   "source": [
    "Note : In the above split we might not have the situation where there exist some movies in test set that are not\n",
    "trained(seen by anyone). We will obviously not be able to predict ratings there. This is clearly evident below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b2e076e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 91/91 [00:00<00:00, 1643.87it/s]\n"
     ]
    }
   ],
   "source": [
    "t,v,tt = get_data(data_dir,[0.6,0.5],'random','sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8aa8d084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(t['user_id'])== set(tt['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7e9c651a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30165289256198347"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tt['movie_id'])-set(t['movie_id']))/len(set(tt['movie_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f197f3",
   "metadata": {},
   "source": [
    "We may be not able to use all this data.Let's make smaller data-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "10fb2077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 91/91 [00:00<00:00, 1564.59it/s]\n"
     ]
    }
   ],
   "source": [
    "data_type = 'sample'\n",
    "split = 'random'\n",
    "t,v,tt = get_data(data_dir,[0.6,0.5],split,data_type,save_to_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "48e2ad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 6743/6743 [00:07<00:00, 921.97it/s]\n"
     ]
    }
   ],
   "source": [
    "data_type = 'partial'\n",
    "t,v,tt = get_data(data_dir,[0.6,0.5],split,data_type,save_to_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4d4e2f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6743, 12649)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(t['user_id'])),len(set(t['movie_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7f72d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(t['user_id']) == set(tt['user_id'])\n",
    "assert set(t['movie_id']) != set(tt['movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9e65262d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07414910858995137"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tt['movie_id'])-set(t['movie_id']))/len(set(tt['movie_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28efdcfa",
   "metadata": {},
   "source": [
    "Note that 7 percent of movies are not there in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8e086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e36544d",
   "metadata": {},
   "source": [
    "## Testing on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329c0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecsysDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,usr_dict=None,mov_dict=None):\n",
    "        self.df = df\n",
    "        self.usr_dict = usr_dict\n",
    "        self.mov_dict = mov_dict\n",
    "    def __getitem__(self,index):\n",
    "        if self.usr_dict and self.mov_dict:\n",
    "            return [self.usr_dict[int(self.df.iloc[index]['user_id'])],self.mov_dict[int(self.df.iloc[index]['movie_id'])]],self.df.iloc[index]['rating']\n",
    "        else:\n",
    "            return [int(self.df.iloc[index]['user_id']-1),int(self.df.iloc[index]['movie_id']-1)],self.df.iloc[index]['rating']\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d3a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame({'user_id':[1,2,3,2,2,3,2,2],'movie_id':[1,2,3,3,3,2,1,1],'rating':[2.0,1.0,4.0,5.0,1.3,3.5,3.0,4.5]})\n",
    "trn_ids = random.sample(range(8),4,)\n",
    "valid_ids = [i for i in range(8) if i not in trn_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ee10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a04effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trn,sample_vld = copy.deepcopy(sample.iloc[trn_ids].reset_index()),copy.deepcopy(sample.iloc[valid_ids].reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c92f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vld = RecsysDataset(sample_vld)\n",
    "sample_trn = RecsysDataset(sample_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1884bdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1], 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vld[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc587e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185b4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(sample_trn, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dacf36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(sample_vld, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dd3f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:tensor([1, 1]),item:tensor([2, 0]) and rating:tensor([5.0000, 4.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for u,r in train_loader:\n",
    "    #user,item = u\n",
    "    print(f'user:{u[0]},item:{u[-1]} and rating:{r}')\n",
    "    #print(u)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f82468c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8380, -0.7193, -0.4033, -0.5966,  0.1820],\n",
       "        [-0.8567,  1.1006, -1.0712,  0.1227, -0.5663],\n",
       "        [-2.1788,  0.5684, -1.0845, -1.3986,  0.4033]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(3, 5)\n",
    "a = torch.tensor([1,2,0])\n",
    "embedding(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00730779",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c535034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    \n",
    "    def __init__(self,user_sz,item_sz,embd_sz,dropout_fac,min_r=0.0,max_r=5.0,alpha=0.5,with_variable_alpha=False):\n",
    "        super().__init__()\n",
    "        self.dropout_fac = dropout_fac\n",
    "        self.user_embd_mtrx = nn.Embedding(user_sz,embd_sz)\n",
    "        self.item_embd_mtrx = nn.Embedding(item_sz,embd_sz)\n",
    "        #bias = torch.zeros(size=(user_sz, 1), requires_grad=True)\n",
    "        self.h =  nn.Linear(embd_sz,1)\n",
    "        self.fst_lyr = nn.Linear(embd_sz*2,embd_sz)\n",
    "        self.snd_lyr = nn.Linear(embd_sz,embd_sz//2)\n",
    "        self.thrd_lyr = nn.Linear(embd_sz//2,embd_sz//4)\n",
    "        self.out_lyr = nn.Linear(embd_sz//4,1)\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.min_r,self.max_r = min_r,max_r\n",
    "        if with_variable_alpha:\n",
    "            self.alpha = torch.tensor(alpha,requires_grad=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        user_emd = self.user_embd_mtrx(x[0])\n",
    "        item_emd = self.item_embd_mtrx(x[-1])\n",
    "        #hadamard-product\n",
    "        gmf = user_emd*item_emd\n",
    "        gmf = self.h(gmf)\n",
    "        \n",
    "        \n",
    "        mlp = torch.cat([user_emd,item_emd],dim=-1)\n",
    "        mlp = self.out_lyr(F.relu(self.thrd_lyr(F.relu(self.snd_lyr(F.dropout(F.relu(self.fst_lyr(mlp)),p=self.dropout_fac))))))\n",
    "        fac = torch.clip(self.alpha,min=0.0,max=1.0)\n",
    "        out = fac*gmf+ (1-fac)*mlp\n",
    "        out = torch.clip(out,min=self.min_r,max=self.max_r)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "257ee1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:tensor([1, 0]),item:tensor([2, 0]) and rating:tensor([5., 2.], dtype=torch.float64)\n",
      "output of the network=> out:tensor([[0.3493],\n",
      "        [0.0404]], grad_fn=<ClampBackward1>),shape:torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "#does it work\n",
    "model = NCF(3,3,4,0.5)\n",
    "for u,r in train_loader:\n",
    "    #user,item = u\n",
    "    print(f'user:{u[0]},item:{u[-1]} and rating:{r}')\n",
    "    #print(u)\n",
    "    out = model(u)\n",
    "    print(f'output of the network=> out:{out},shape:{out.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae1145",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5442585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, device,loss_fn=None, optimizer=None, scheduler=None,artifacts_loc=None,exp_tracker=None):\n",
    "\n",
    "        # Set params\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.store_loc = artifacts_loc\n",
    "        self.exp_tracker = exp_tracker\n",
    "\n",
    "    def train_step(self, dataloader):\n",
    "        \"\"\"Train step.\"\"\"\n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over train batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            #batch = [item.to(self.device) for item in batch]  # Set device\n",
    "            inputs,targets = batch\n",
    "            inputs = [item.to(self.device) for item in inputs]\n",
    "            targets = targets.to(self.device)\n",
    "            #inputs, targets = batch[:-1], batch[-1]\n",
    "            #import pdb;pdb.set_trace()\n",
    "            self.optimizer.zero_grad()  # Reset gradients\n",
    "            z = self.model(inputs)  # Forward pass\n",
    "            targets = targets.reshape(z.shape)\n",
    "            J = self.loss_fn(z.float(), targets.float())  # Define loss\n",
    "            J.backward()  # Backward pass\n",
    "            self.optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulative Metrics\n",
    "            loss += (J.detach().item() - loss) / (i + 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_step(self, dataloader):\n",
    "        \"\"\"Validation or test step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        y_trues, y_probs = [], []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.inference_mode():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                inputs,y_true = batch\n",
    "                inputs = [item.to(self.device) for item in inputs]\n",
    "                y_true = y_true.to(self.device).float()\n",
    "\n",
    "                # Step\n",
    "                z = self.model(inputs).float()  # Forward pass\n",
    "                y_true = y_true.reshape(z.shape)\n",
    "                J = self.loss_fn(z, y_true).item()\n",
    "\n",
    "                # Cumulative Metrics\n",
    "                loss += (J - loss) / (i + 1)\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = z.cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "                y_trues.extend(y_true.cpu().numpy())\n",
    "\n",
    "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "\n",
    "    def predict_step(self, dataloader):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        # Set model to eval mode\n",
    "        self.model.eval()\n",
    "        y_probs = []\n",
    "\n",
    "        # Iterate over val batches\n",
    "        with torch.inference_mode():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # Forward pass w/ inputs\n",
    "                inputs, targets = batch\n",
    "                z = self.model(inputs).float()\n",
    "\n",
    "                # Store outputs\n",
    "                y_prob = z.cpu().numpy()\n",
    "                y_probs.extend(y_prob)\n",
    "\n",
    "        return np.vstack(y_probs)\n",
    "    \n",
    "    def train(self, num_epochs, patience, train_dataloader, val_dataloader, \n",
    "              tolerance=1e-5):\n",
    "        best_val_loss = np.inf\n",
    "        training_stats = defaultdict(list)\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            # Steps\n",
    "            train_loss = self.train_step(dataloader=train_dataloader)\n",
    "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
    "            #store stats\n",
    "            training_stats['epoch'].append(epoch)\n",
    "            training_stats['train_loss'].append(train_loss)\n",
    "            training_stats['val_loss'].append(val_loss)\n",
    "            #log-stats\n",
    "            if self.exp_tracker == 'wandb':\n",
    "                log_metrics = {'epoch':epoch,'train_loss':train_loss,'val_loss':val_loss}\n",
    "                wandb.log(log_metrics,step=epoch)\n",
    "            \n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss - tolerance:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = self.model\n",
    "                _patience = patience  # reset _patience\n",
    "            else:\n",
    "                _patience -= 1\n",
    "            if not _patience:  # 0\n",
    "                print(\"Stopping early!\")\n",
    "                break\n",
    "\n",
    "            # Tracking\n",
    "            #mlflow.log_metrics({\"train_loss\": train_loss, \"val_loss\": val_loss}, step=epoch)\n",
    "\n",
    "            # Logging\n",
    "            if epoch%5 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1} | \"\n",
    "                    f\"train_loss: {train_loss:.5f}, \"\n",
    "                    f\"val_loss: {val_loss:.5f}, \"\n",
    "                    f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
    "                    f\"_patience: {_patience}\"\n",
    "                )\n",
    "        if self.store_loc:\n",
    "            pd.DataFrame(training_stats).to_csv(self.store_loc/'training_stats.csv',index=False)\n",
    "        return best_model, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db3a768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=5)\n",
    "\n",
    "trainer = Trainer(model,'cpu',loss_fn,optimizer,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc6b0ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 443.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 11.97598, val_loss: 6.06628, lr: 1.00E-03, _patience: 10\n",
      "Epoch: 11 | train_loss: 11.71282, val_loss: 5.94650, lr: 1.00E-03, _patience: 10\n",
      "Epoch: 21 | train_loss: 11.45816, val_loss: 5.83341, lr: 1.00E-03, _patience: 10\n",
      "Epoch: 31 | train_loss: 11.21199, val_loss: 5.72781, lr: 1.00E-03, _patience: 10\n",
      "Epoch: 41 | train_loss: 10.96701, val_loss: 5.62273, lr: 1.00E-03, _patience: 10\n",
      "Epoch: 51 | train_loss: 10.70814, val_loss: 5.51317, lr: 1.00E-03, _patience: 9\n",
      "Epoch: 61 | train_loss: 10.41593, val_loss: 5.38095, lr: 1.00E-03, _patience: 10\n",
      "Epoch: 71 | train_loss: 10.07872, val_loss: 5.30314, lr: 1.00E-03, _patience: 9\n",
      "Epoch: 81 | train_loss: 9.57599, val_loss: 5.07092, lr: 1.00E-03, _patience: 10\n",
      "Epoch: 91 | train_loss: 9.02137, val_loss: 4.92003, lr: 1.00E-03, _patience: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(NCF(\n",
       "   (user_embd_mtrx): Embedding(3, 4)\n",
       "   (item_embd_mtrx): Embedding(3, 4)\n",
       "   (h): Linear(in_features=4, out_features=1, bias=True)\n",
       "   (fst_lyr): Linear(in_features=8, out_features=4, bias=True)\n",
       "   (snd_lyr): Linear(in_features=4, out_features=2, bias=True)\n",
       "   (thrd_lyr): Linear(in_features=2, out_features=1, bias=True)\n",
       "   (out_lyr): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " 4.7468710243701935)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(100,10,train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362199a3",
   "metadata": {},
   "source": [
    "The code -> Network+trainingloop works. Let's write experiment tracking system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df31178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    #main function that handles everything.\n",
    "    config_dict = vars(args)\n",
    "    if args.exp_tracker == 'wandb':\n",
    "        wandb.init(project=f\"{args.trail_id}_{args.dataset}_{args.data_type}\",config=config_dict)\n",
    "        \n",
    "    base_dir = Path(args.base_dir)\n",
    "    if not args.base_dir:\n",
    "        base_dir = Path(os.getcwd())\n",
    "        \n",
    "    if args.dataset == 'movielens':\n",
    "        data_dir = base_dir/'data'/'archive'\n",
    "    else:\n",
    "        Print('Unknown dataset')\n",
    "        exit()\n",
    " \n",
    "    store_dir = base_dir/'artifacts'\n",
    "    base_dir = args.base_dir\n",
    "    os.makedirs(store_dir,exist_ok=True)\n",
    "    \n",
    "    #Check if all the necessary data is already there\n",
    "    is_data_premade = True\n",
    "    for d_type in args.on:\n",
    "        file_name = args.data_type+'_'+args.split_type+'_'+d_type+'.csv'\n",
    "        if file_name not in os.listdir(data_dir):\n",
    "            is_data_premade=False\n",
    "   \n",
    "    #load the dataset\n",
    "    df = pd.read_csv(data_dir/(args.data_type+'_'+'rating.csv'))\n",
    "    mov_dict = dict(zip(sorted(set(df['movieId'])),range(len(sorted(set(df['movieId']))))))\n",
    "    usr_dict = dict(zip(sorted(set(df['userId'])),range(len(sorted(set(df['userId']))))))\n",
    "        \n",
    "    if not is_data_premade:\n",
    "        pass\n",
    "        #get-data\n",
    "        #?\n",
    "        #trn_df,vld_df,tst_df = get_data()\n",
    "    else:\n",
    "        #load the csv's\n",
    "        trn_file= args.data_type+'_'+args.split_type+'_'+'trn'+'.csv'\n",
    "        vld_file = args.data_type+'_'+args.split_type+'_'+'vld'+'.csv'\n",
    "        tst_file = args.data_type+'_'+args.split_type+'_'+'tst'+'.csv'\n",
    "        trn_df,vld_df,tst_df = pd.read_csv(data_dir/trn_file),pd.read_csv(data_dir/vld_file),pd.read_csv(data_dir/tst_file)\n",
    "        \n",
    "    train_artifacts_loc = store_dir/(str(args.trail_id)+'_'+args.dataset+'_'+args.data_type)\n",
    "    exp_tracker = args.exp_tracker\n",
    "    if args.mode == 'train':\n",
    "         \n",
    "        #train-from scratch\n",
    "        trn = RecsysDataset(trn_df,usr_dict,mov_dict)\n",
    "        vld = RecsysDataset(vld_df,usr_dict,mov_dict)\n",
    "        tst = RecsysDataset(tst_df,usr_dict,mov_dict)\n",
    "        \n",
    "        trn_dl,vld_dl,tst_dl = dl(trn, batch_size=args.batch_size, shuffle=True),dl(vld, batch_size=args.batch_size),dl(tst, batch_size=args.batch_size)\n",
    "        \n",
    "        #get model\n",
    "        emb_len_usr = len(set(df['userId']))\n",
    "        emb_len_itm = len(set(df['movieId']))\n",
    "        model = NCF(emb_len_usr,emb_len_itm,args.embd_sz,args.dropout_p,alpha=args.alpha,with_variable_alpha=args.with_variable_alpha)\n",
    "        \n",
    "        device = 'cpu'\n",
    "        loss_fn = nn.MSELoss()\n",
    "        # Define optimizer & scheduler\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=5)\n",
    "        \n",
    "        \n",
    "        os.makedirs(train_artifacts_loc,exist_ok=True)\n",
    "        \n",
    "        #train\n",
    "        trainer = Trainer(model,'cpu',loss_fn,optimizer,scheduler,artifacts_loc=train_artifacts_loc,exp_tracker=exp_tracker)\n",
    "        best_model, best_val_loss = trainer.train(args.num_epochs,args.patience,trn_dl,vld_dl)\n",
    "        #save the model\n",
    "        PATH = train_artifacts_loc/'model.pth'\n",
    "        torch.save(best_model.state_dict(), PATH)\n",
    "        \n",
    "        test_ratings = tst_df['rating']\n",
    "        #loss, np.vstack(y_trues), np.vstack(y_probs)\n",
    "        tst_loss,y_trues,y_predict = trainer.eval_step(tst_dl)\n",
    "        test_rslts = pd.DataFrame({'y_true':y_trues.squeeze(),'y_pred':y_predict.squeeze()},index=range(len(y_trues)))\n",
    "        \n",
    "        #also do for train-dataset\n",
    "        trn_loss,y_trues_trn,y_predict_trn = trainer.eval_step(trn_dl)\n",
    "        trn_rslts = pd.DataFrame({'y_true':y_trues_trn.squeeze(),'y_pred':y_predict_trn.squeeze()},index=range(len(y_trues_trn)))\n",
    "        \n",
    "        #also do for valid-dataset\n",
    "        vld_loss,y_trues_vld,y_predict_vld = trainer.eval_step(trn_dl)\n",
    "        vld_rslts = pd.DataFrame({'y_true':y_trues_vld.squeeze(),'y_pred':y_predict_vld.squeeze()},index=range(len(y_trues_vld)))\n",
    "        \n",
    "        #save test predictions\n",
    "        test_rslts.to_csv(str(train_artifacts_loc/('inference'+'_'+args.split_type+'_tst'+'.csv')))\n",
    "        #save trn predictions\n",
    "        trn_rslts.to_csv(str(train_artifacts_loc/('inference'+'_'+args.split_type+'_trn'+'.csv')))\n",
    "        vld_rslts.to_csv(str(train_artifacts_loc/('inference'+'_'+args.split_type+'_vld'+'.csv')))\n",
    "        dump(args,train_artifacts_loc/'args.joblib')\n",
    "        \n",
    "    if args.mode == 'inference':\n",
    "        #? This code is not written yet - buggy\n",
    "        losses = {}\n",
    "        for split_type in args.on:\n",
    "            #'trn','vld','tst'\n",
    "            if split_type == 'trn':\n",
    "                data = RecsysDataset(trn_df)\n",
    "            elif split_type == 'vld':\n",
    "                data = RecsysDataset(vld_df)\n",
    "            elif split_type == 'tst':\n",
    "                data = RecsysDataset(tst_df)\n",
    "            data_dl = dl(data, batch_size=args.batch_size)\n",
    "            #?load it from the path\n",
    "            dir_loc = store_dir/(str(trail_id)+'_'+dataset+'_'+data_type)\n",
    "            #model  = \n",
    "            trainer = Trainer(model,'cpu',loss_fn,optimizer,scheduler,artifacts_loc=train_artifacts_loc)\n",
    "            tst_loss,y_trues,y_predict = trainer.eval_step(data_dl)\n",
    "            test_rslts = pd.DataFrame({'y_true':y_trues.squeeze(),'y_pred':y_predict})\n",
    "            \n",
    "            losses[split_type] = tst_loss\n",
    "            test_rslts.to_csv(str(dir_loc)/('inference'+'_'+split_type+'.csv'))\n",
    "                \n",
    "     \n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06b5df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = pd.read_csv(data_dir/'partial_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d570765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset has 13950 movies and 6743 users'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"dataset has {len(set(df_part['movieId']))} movies and {len(set(df_part['userId']))} users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a286443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce4780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75956d4c",
   "metadata": {},
   "source": [
    "Below I we will study 3 models and their performance\n",
    "\n",
    "- alpha == 1,CONSTANT ALPHA -> GMF (1)\n",
    "- alpha == 0,CONSTANT ALPHA -> MLP(2)\n",
    "- alpha == 0.5,Constant alpha -> equal contribution(3)\n",
    "- alpha == 0.5,variable alpha-> dynamically figure out the value of optimal value(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "982f6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha == 1,CONSTANT ALPHA -> GMF (trail_id=1)\n",
    "\n",
    "#mode = 'train','inference'\n",
    "#data_type = 'sample','full','partial'\n",
    "\n",
    "args_1 = Namespace(\n",
    "    exp_tracker = 'wandb',\n",
    "    base_dir = '/Users/vinay/Projects/Recsys',\n",
    "    model_path = '',\n",
    "    trail_id = 1,\n",
    "    dataset = 'movielens',\n",
    "    mode = 'train',\n",
    "    on = ['trn','vld','tst'],\n",
    "    alpha=1.0,\n",
    "    with_variable_alpha=False,\n",
    "    data_type = 'partial',\n",
    "    split_type = 'random',\n",
    "    split = [0.6,0.5],\n",
    "    embd_sz = 32,\n",
    "    batch_size=64, \n",
    "    dropout_p=0.5,\n",
    "    lr=2e-4,\n",
    "    num_epochs=100,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "args_2 = Namespace(\n",
    "    exp_tracker = 'wandb',\n",
    "    base_dir = '/Users/vinay/Projects/Recsys',\n",
    "    model_path = '',\n",
    "    trail_id = 2,\n",
    "    dataset = 'movielens',\n",
    "    mode = 'train',\n",
    "    on = ['trn','vld','tst'],\n",
    "    alpha=0.0,\n",
    "    with_variable_alpha=False,\n",
    "    data_type = 'partial',\n",
    "    split_type = 'random',\n",
    "    split = [0.6,0.5],\n",
    "    embd_sz = 32,\n",
    "    batch_size=64, \n",
    "    dropout_p=0.5,\n",
    "    lr=2e-4,\n",
    "    num_epochs=100,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "args_3 = Namespace(\n",
    "    exp_tracker = 'wandb',\n",
    "    base_dir = '/Users/vinay/Projects/Recsys',\n",
    "    model_path = '',\n",
    "    trail_id = 3,\n",
    "    dataset = 'movielens',\n",
    "    mode = 'train',\n",
    "    on = ['trn','vld','tst'],\n",
    "    alpha=0.5,\n",
    "    with_variable_alpha=False,\n",
    "    data_type = 'partial',\n",
    "    split_type = 'random',\n",
    "    split = [0.6,0.5],\n",
    "    embd_sz = 32,\n",
    "    batch_size=64, \n",
    "    dropout_p=0.5,\n",
    "    lr=2e-4,\n",
    "    num_epochs=100,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "\n",
    "args_4 = Namespace(\n",
    "    exp_tracker = 'wandb',\n",
    "    base_dir = '/Users/vinay/Projects/Recsys',\n",
    "    model_path = '',\n",
    "    trail_id = 4,\n",
    "    dataset = 'movielens',\n",
    "    mode = 'train',\n",
    "    on = ['trn','vld','tst'],\n",
    "    alpha=0.5,\n",
    "    with_variable_alpha=True,\n",
    "    data_type = 'partial',\n",
    "    split_type = 'random',\n",
    "    split = [0.6,0.5],\n",
    "    embd_sz = 32,\n",
    "    batch_size=128, \n",
    "    dropout_p=0.5,\n",
    "    lr=2e-4,\n",
    "    num_epochs=100,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61089f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(alpha=1.0, base_dir='/Users/vinay/Projects/Recsys', batch_size=64, data_type='partial', dataset='movielens', dropout_p=0.5, embd_sz=32, exp_tracker='wandb', lr=0.0002, mode='train', model_path='', num_epochs=100, on=['trn', 'vld', 'tst'], patience=10, split=[0.6, 0.5], split_type='random', trail_id=1, with_variable_alpha=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b63efe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvin136\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vin136/1_movielens_partial/runs/2iqpmpfo\" target=\"_blank\">zesty-energy-5</a></strong> to <a href=\"https://wandb.ai/vin136/1_movielens_partial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                       | 1/100 [01:49<3:00:54, 109.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 8.38704, val_loss: 5.54270, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                     | 6/100 [10:39<2:46:06, 106.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 1.10523, val_loss: 1.11294, lr: 2.00E-04, _patience: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▎                                  | 11/100 [19:26<2:36:31, 105.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss: 1.09611, val_loss: 1.12085, lr: 2.00E-05, _patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▋                                  | 12/100 [22:58<2:48:27, 114.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run(args_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17d279aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2iqpmpfo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10655... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>train_loss</td><td>1.09541</td></tr><tr><td>val_loss</td><td>1.12162</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">zesty-energy-5</strong>: <a href=\"https://wandb.ai/vin136/1_movielens_partial/runs/2iqpmpfo\" target=\"_blank\">https://wandb.ai/vin136/1_movielens_partial/runs/2iqpmpfo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211202_130614-2iqpmpfo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2iqpmpfo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vin136/2_movielens_partial/runs/10x1rgpy\" target=\"_blank\">cosmic-surf-3</a></strong> to <a href=\"https://wandb.ai/vin136/2_movielens_partial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                      | 1/100 [10:39<17:34:22, 639.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.61496, val_loss: 1.08814, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                     | 6/100 [20:17<3:42:25, 141.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.83313, val_loss: 0.83841, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▎                                  | 11/100 [29:11<2:46:55, 112.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss: 0.76910, val_loss: 0.79568, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▏                                | 16/100 [38:04<2:30:21, 107.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | train_loss: 0.74284, val_loss: 0.77361, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▏                              | 21/100 [46:55<2:20:07, 106.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | train_loss: 0.72822, val_loss: 0.76295, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▏                            | 26/100 [55:50<2:11:42, 106.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | train_loss: 0.71906, val_loss: 0.76202, lr: 2.00E-04, _patience: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▍                         | 31/100 [1:31:41<6:04:45, 317.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | train_loss: 0.71264, val_loss: 0.75723, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▎                       | 36/100 [1:40:36<2:31:39, 142.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | train_loss: 0.70780, val_loss: 0.75592, lr: 2.00E-04, _patience: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████▏                     | 41/100 [1:49:31<1:51:07, 113.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | train_loss: 0.70361, val_loss: 0.75714, lr: 2.00E-04, _patience: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████                    | 46/100 [1:58:26<1:37:06, 107.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | train_loss: 0.70077, val_loss: 0.76179, lr: 2.00E-04, _patience: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|██████████████████▊                  | 51/100 [2:22:00<3:58:34, 292.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | train_loss: 0.69142, val_loss: 0.75424, lr: 2.00E-05, _patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████▋                | 56/100 [2:31:02<1:42:19, 139.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | train_loss: 0.68999, val_loss: 0.75318, lr: 2.00E-05, _patience: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████▌              | 61/100 [2:40:02<1:13:35, 113.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | train_loss: 0.68844, val_loss: 0.75427, lr: 2.00E-06, _patience: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████▌              | 61/100 [2:41:50<1:43:28, 159.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:10x1rgpy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11741... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>61</td></tr><tr><td>train_loss</td><td>0.68889</td></tr><tr><td>val_loss</td><td>0.7533</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cosmic-surf-3</strong>: <a href=\"https://wandb.ai/vin136/2_movielens_partial/runs/10x1rgpy\" target=\"_blank\">https://wandb.ai/vin136/2_movielens_partial/runs/10x1rgpy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211202_133146-10x1rgpy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:10x1rgpy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vin136/3_movielens_partial/runs/3tazumoq\" target=\"_blank\">cool-cherry-2</a></strong> to <a href=\"https://wandb.ai/vin136/3_movielens_partial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                       | 1/100 [01:46<2:55:23, 106.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.73742, val_loss: 1.09607, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                       | 2/100 [03:33<2:54:18, 106.72s/it]wandb: Network error (ConnectionError), entering retry loop.\n",
      "  6%|██▏                                  | 6/100 [1:06:11<17:13:12, 659.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.83325, val_loss: 0.83875, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████                                 | 11/100 [1:15:13<4:44:08, 191.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss: 0.76169, val_loss: 0.78499, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████▉                               | 16/100 [1:24:18<2:51:54, 122.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | train_loss: 0.73569, val_loss: 0.76714, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▊                             | 21/100 [1:33:15<2:24:30, 109.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | train_loss: 0.71912, val_loss: 0.75746, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████▌                           | 26/100 [1:42:13<2:13:03, 107.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | train_loss: 0.71021, val_loss: 0.75357, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▍                         | 31/100 [1:51:05<2:02:33, 106.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | train_loss: 0.70306, val_loss: 0.75139, lr: 2.00E-04, _patience: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▎                       | 36/100 [2:03:25<2:59:51, 168.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | train_loss: 0.69734, val_loss: 0.74907, lr: 2.00E-04, _patience: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████▏                     | 41/100 [2:12:19<1:55:23, 117.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | train_loss: 0.69165, val_loss: 0.74875, lr: 2.00E-05, _patience: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████                    | 46/100 [2:21:15<1:37:50, 108.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | train_loss: 0.68290, val_loss: 0.74689, lr: 2.00E-05, _patience: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|██████████████████▊                  | 51/100 [2:30:05<1:26:55, 106.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | train_loss: 0.68166, val_loss: 0.74721, lr: 2.00E-06, _patience: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████▋                | 56/100 [2:39:05<1:19:06, 107.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | train_loss: 0.68133, val_loss: 0.74757, lr: 2.00E-06, _patience: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████▌              | 61/100 [2:48:02<1:09:41, 107.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | train_loss: 0.68093, val_loss: 0.74607, lr: 2.00E-06, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████▍            | 66/100 [2:56:59<1:00:45, 107.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | train_loss: 0.68127, val_loss: 0.74638, lr: 2.00E-06, _patience: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████▉           | 70/100 [3:05:57<1:19:41, 159.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3tazumoq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16534... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>70</td></tr><tr><td>train_loss</td><td>0.6812</td></tr><tr><td>val_loss</td><td>0.74726</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-cherry-2</strong>: <a href=\"https://wandb.ai/vin136/3_movielens_partial/runs/3tazumoq\" target=\"_blank\">https://wandb.ai/vin136/3_movielens_partial/runs/3tazumoq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211202_161620-3tazumoq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3tazumoq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vin136/4_movielens_partial/runs/dk8k2k6w\" target=\"_blank\">earthy-jazz-2</a></strong> to <a href=\"https://wandb.ai/vin136/4_movielens_partial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                        | 1/100 [01:36<2:38:51, 96.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 2.34668, val_loss: 1.14362, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                      | 6/100 [09:39<2:31:07, 96.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.90191, val_loss: 0.89937, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▍                                   | 11/100 [17:36<2:21:47, 95.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | train_loss: 0.79898, val_loss: 0.81825, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▍                                 | 16/100 [25:34<2:13:45, 95.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | train_loss: 0.76106, val_loss: 0.79204, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▍                               | 21/100 [33:32<2:05:46, 95.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | train_loss: 0.74149, val_loss: 0.77620, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▍                             | 26/100 [41:30<1:57:55, 95.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | train_loss: 0.73098, val_loss: 0.77035, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████                           | 31/100 [54:57<3:43:09, 194.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | train_loss: 0.72467, val_loss: 0.76732, lr: 2.00E-04, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▎                       | 36/100 [1:03:08<2:02:20, 114.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | train_loss: 0.71883, val_loss: 0.76680, lr: 2.00E-04, _patience: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████▏                     | 41/100 [1:11:16<1:38:39, 100.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | train_loss: 0.71447, val_loss: 0.76860, lr: 2.00E-04, _patience: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████▍                    | 46/100 [1:19:25<1:28:57, 98.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | train_loss: 0.70849, val_loss: 0.76384, lr: 2.00E-04, _patience: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|██████████████████▊                  | 51/100 [1:28:19<1:25:57, 105.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | train_loss: 0.69951, val_loss: 0.76228, lr: 2.00E-05, _patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████▋                | 56/100 [1:37:10<1:17:52, 106.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | train_loss: 0.69867, val_loss: 0.76240, lr: 2.00E-05, _patience: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████▌              | 61/100 [1:46:05<1:09:39, 107.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | train_loss: 0.69753, val_loss: 0.76207, lr: 2.00E-05, _patience: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████▍             | 66/100 [1:54:24<56:30, 99.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | train_loss: 0.69737, val_loss: 0.76260, lr: 2.00E-05, _patience: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████▍           | 71/100 [2:02:25<46:46, 96.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | train_loss: 0.69667, val_loss: 0.76193, lr: 2.00E-06, _patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████           | 72/100 [2:05:42<48:53, 104.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run(args_2)\n",
    "run(args_3)\n",
    "run(args_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc607c9",
   "metadata": {},
   "source": [
    "## To DO-> COMPARE TEST PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f115494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv('/Users/vinay/Projects/Recsys/artifacts/0_movielens_sample/inference_random_tst.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "514eb6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.289770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.038643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.109258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.687514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.484389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>2088</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.660824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>2089</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.312373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>2090</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.657814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>2091</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.827990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>2092</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.686951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  y_true    y_pred\n",
       "0              0     4.0  4.289770\n",
       "1              1     3.5  4.038643\n",
       "2              2     4.0  4.109258\n",
       "3              3     3.5  3.687514\n",
       "4              4     3.5  4.484389\n",
       "...          ...     ...       ...\n",
       "2088        2088     2.5  3.660824\n",
       "2089        2089     0.5  3.312373\n",
       "2090        2090     3.5  3.657814\n",
       "2091        2091     4.0  3.827990\n",
       "2092        2092     4.0  3.686951\n",
       "\n",
       "[2093 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918899c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecce2dca",
   "metadata": {},
   "source": [
    "- IF we recommend based on `y_pred`(define some notion of `high` and `low` rating..say all those that are greater than some value)and compare between various models.\n",
    "- A good way to choose this threshold is use `valid` results and choose the metric.\n",
    "- other simple way is just look at the squared error on test set and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e22ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
